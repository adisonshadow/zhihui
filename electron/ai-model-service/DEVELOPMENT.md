# AI 模型服务 - 开发说明

## 1. 背景与目标

当前抠图模型（BiRefNet、MVANet、RVM、U2-Net 等）直接内嵌在 `spriteOnnxService.ts` 中，存在：

- **难以定位问题**：某模型失败时，日志与主流程混在一起，难以判断是预处理、模型调用还是后处理出错
- **扩展成本高**：新增模型需改动多处，易产生遗漏与回归
- **内存与稳定性**：大模型易 OOM，影响主进程；单点崩溃波及整个应用
- **可测试性差**：难以单独验证某个模型的输入输出是否符合预期

通过独立的 **AI 模型服务层**，可统一接口、集中错误处理、支持进程隔离，便于排查与扩展。

---

## 2. 推荐架构

### 2.1 目录与职责

```
electron/ai-model-service/
├── DEVELOPMENT.md          # 本文档
├── types.ts                # 统一类型定义
├── registry.ts             # 模型注册与调度
├── base.ts                 # 抽象基类 / 接口约束
├── runner.ts               # 进程 / 线程编排（可选）
└── adapters/               # 各模型适配器
    ├── rvm.ts
    ├── birefnet.ts
    ├── mvanet.ts
    ├── u2netp.ts
    └── ...
```

- **types**：输入输出、模型元信息、错误码等
- **registry**：按名称查找并调用对应 adapter
- **base**：统一预处理/后处理、日志、异常封装
- **runner**：决定在主进程运行，还是子进程 / Worker
- **adapters**：每个模型独立文件，实现统一接口

### 2.2 统一模型接口

每个模型适配器实现同一接口，包含：

- **输入**：RGBA/RGB Buffer、宽、高、可选参数（如下采样比、是否 Alpha Matting）
- **输出**：RGBA Buffer（原图分辨率）或结构化结果
- **元信息**：模型 ID、所需输入尺寸、是否支持动态尺寸等

调用方只需传入模型 ID 和输入，不关心具体实现。

---

## 3. 健壮性设计

### 3.1 输入校验

- 在进入 adapter 前统一校验：尺寸、通道数、Buffer 长度
- 不符合时提前返回明确错误（如「输入尺寸必须为正整数」），避免进入 ONNX 后出现模糊报错

### 3.2 模型加载隔离

- 首次使用时懒加载，失败时记录并标记为「不可用」
- 支持按需预加载或预热，避免首次请求超时
- 模型路径可从配置读取，支持多目录、多版本

### 3.3 超时与资源限制

- 每次推理设置超时（如 60s）
- 子进程模式下通过 `--max-old-space-size` 限制内存
- 超时或内存超限时，终止子进程并返回统一错误码

### 3.4 错误分层

- **可恢复**：输入错误、模型文件缺失 → 返回业务错误，不崩溃
- **不可恢复**：ONNX 内部崩溃 → 在子进程中捕获，主进程收到错误对象而非未处理异常
- 所有错误带错误码、简要描述，便于日志聚合与用户提示

### 3.5 日志与可观测性

- 每次调用记录：模型 ID、输入尺寸、耗时、成功/失败
- 失败时记录：错误类型、堆栈（开发环境）、相关参数
- 便于后续做性能分析与问题定位

---

## 4. 进程隔离策略

### 4.1 何时使用子进程

- **建议使用子进程**：BiRefNet、MVANet 等大模型、易 OOM 的场景
- **可在主进程**：U2NetP、RVM 等小模型

### 4.2 子进程通信

- 通过 stdin/stdout 传输 JSON，或使用 Node `child_process` + 结构化消息
- 传入：项目目录、图片路径或 Base64、模型 ID、参数
- 传出：结果路径、RGBA Buffer、或错误对象
- 避免传输大 Buffer 时，可写临时文件，仅传路径

### 4.3 子进程生命周期

- **按需创建**：首次调用某模型时启动
- **复用**：同一模型可复用同一子进程，减少启动开销
- **空闲回收**：一段时间无请求后关闭，避免常驻占用内存

---

## 5. 新增模型流程

1. 在 `adapters/` 下新建适配器文件
2. 实现统一接口（预处理、推理、后处理）
3. 在 `registry` 中注册模型 ID
4. 适配器内部可包含：输入尺寸、输入输出名称、归一化方式等，便于后续调参与排错
5. 优先在隔离环境（如单独脚本）验证预处理与推理是否正确，再接入服务

---

## 6. 调试与排错

### 6.1 独立验证脚本

- 为每个模型提供独立 CLI 脚本，接受单张图片路径，输出 mask 或 RGBA
- 用于快速验证：预处理、ONNX 输入输出名称与形状、后处理逻辑

### 6.2 输入输出快照

- 开发模式下，可将关键步骤的输入输出保存到临时目录（如 tensor shape、归一化前后抽样）
- 便于与 Python/官方实现对比，快速定位差异

### 6.3 与官方实现对照

- 对照官方 Python/C++ 示例，逐项检查：resize 尺寸、归一化参数、NCHW 顺序、输出名、sigmoid 等
- 文档化每个模型的「已验证配置」，减少后续改动引入回归

---

## 7. 最佳实现顺序

1. **定义 types + 统一接口**，确保与现有 `spriteOnnxService` 调用方式兼容
2. **抽取一个已有模型（如 U2NetP）** 做成 adapter，验证接口设计
3. **实现 registry**，支持按模型 ID 调度
4. **逐步迁移** RVM、BiRefNet、MVANet 等，每迁移一个即验证功能
5. **引入 runner**，对大模型启用子进程隔离
6. **增加超时、日志、错误码**，完善健壮性
7. **提供独立验证脚本**，方便后续新增模型时自测

---

## 8. 与现有代码的衔接

- `spriteOnnxService.processSpriteWithOnnx` 可改为调用 `ai-model-service` 的 registry
- 传入模型 ID 和每帧数据，由 registry 分发到对应 adapter
- 保持对外的 `ProcessSpriteOptions`、返回值格式不变，实现渐进式迁移
